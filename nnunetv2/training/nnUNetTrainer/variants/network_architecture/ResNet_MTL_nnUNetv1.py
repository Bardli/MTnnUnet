import torch
import torch.nn as nn
from typing import Tuple, Union, List, Type
import os

# ---------------------------------------------------------------------
# 1. å¯¼å…¥ nnU-Net v2 çš„æ ¸å¿ƒæ„å»ºæ¨¡å—
# ---------------------------------------------------------------------
from dynamic_network_architectures.building_blocks.residual import StackedResidualBlocks, BasicBlockD
from dynamic_network_architectures.building_blocks.simple_conv_blocks import StackedConvBlocks
from dynamic_network_architectures.building_blocks.helper import get_matching_convtransp, maybe_convert_scalar_to_list
# --- GradNorm START: æ·»åŠ æ–° imports ---
from torch.autograd import grad
import torch.nn.functional as F
# ---------------------------------------------------------------------
# 2. å¤ç”¨ ClassificationHead å’Œ CrossAttentionPooling
# (è¿™éƒ¨åˆ†ä»£ç æ— éœ€ä¿®æ”¹)
# ---------------------------------------------------------------------
class CrossAttentionPooling(nn.Module):
    def __init__(self, embed_dim, query_num, num_classes, num_heads=4, dropout=0.0):
        super(CrossAttentionPooling, self).__init__()
        self.embed_dim = embed_dim
        self.num_classes = num_classes
        self.query_num = query_num
        self.class_query = nn.Parameter(torch.randn(query_num, embed_dim))
        self.cross_attention = nn.MultiheadAttention(
            embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, batch_first=False
        )
        self.norm = nn.LayerNorm(embed_dim)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(query_num * embed_dim, num_classes)
        self._init_weights()
    def _init_weights(self):
        nn.init.xavier_uniform_(self.class_query)
        nn.init.xavier_uniform_(self.classifier.weight)
        nn.init.constant_(self.classifier.bias, 0)
    def forward(self, x):
        b = x.shape[0]
        if x.dim() == 5: x = x.flatten(2)
        x = x.permute(2, 0, 1) # [L, B, D]
        query = self.class_query.unsqueeze(1).repeat(1, b, 1) # [query_num, B, D]
        attended, _ = self.cross_attention(query=query, key=x, value=x)
        attended = self.norm(attended)
        attended = self.dropout(attended)
        attended_permuted = attended.permute(1, 0, 2) # [B, query_num, D]
        attended_flatten = attended_permuted.flatten(1) # [B, query_num * D]
        return self.classifier(attended_flatten)

class ClassificationHead(nn.Module):
    def __init__(self, embed_dim, query_num, num_classes, dropout=0.0, use_cross_attention=True, num_heads=4):
        super(ClassificationHead, self).__init__()
        if use_cross_attention:
            self.pooling = CrossAttentionPooling(
                embed_dim, query_num, num_classes, num_heads, dropout
            )
        else:
            self.pooling = nn.Sequential(
                nn.AdaptiveAvgPool3d(1), nn.Flatten(1), nn.Dropout(dropout),
                nn.Linear(embed_dim, num_classes)
            )
    def forward(self, x):
        return self.pooling(x)

# ---------------------------------------------------------------------
# 3. ä¿®æ­£åçš„å¤šä»»åŠ¡æ¨¡å‹ (ç»§æ‰¿è‡ª nn.Module)
# ---------------------------------------------------------------------

class ResNet_MTL_nnUNet(nn.Module):
    def __init__(
        self,
        # --- å¿…éœ€å‚æ•° (æ— é»˜è®¤å€¼) ---
        input_channels: int,
        n_stages: int,
        features_per_stage: Tuple[int, ...],
        conv_op: Type[nn.Module],
        kernel_sizes: Union[int, Tuple[int, ...], Tuple[Tuple[int, ...], ...]],
        strides: Union[int, Tuple[int, ...], Tuple[Tuple[int, ...], ...]],
        n_blocks_per_stage: Union[int, Tuple[int, ...], Tuple[Tuple[int, ...], ...]],
        num_classes: int, # åˆ†å‰²ä»»åŠ¡çš„ç±»åˆ«æ•°
        n_conv_per_stage_decoder: Union[int, Tuple[int, ...], Tuple[Tuple[int, ...], ...]],
        
        # --- â˜…â˜…â˜… ä¿®æ­£ç‚¹ â˜…â˜…â˜… ---
        # æ–°å¢çš„å¿…éœ€å‚æ•° (åˆ†ç±»ç±»åˆ«æ•°)
        cls_num_classes: int, 
        # ------------------------

        # --- å¯é€‰å‚æ•° (æœ‰é»˜è®¤å€¼) ---
        conv_bias: bool = False,
        norm_op: Type[nn.Module] = nn.InstanceNorm3d,
        norm_op_kwargs: dict = None,
        dropout_op: Type[nn.Module] = nn.Dropout3d,
        dropout_op_kwargs: dict = None,
        nonlin: Type[nn.Module] = nn.LeakyReLU,
        nonlin_kwargs: dict = None,
        deep_supervision: bool = True,
        
        # --- ResNet ç‰¹å®šå¯é€‰å‚æ•° ---
        block: Type[nn.Module] = BasicBlockD,
        stem_channels: int = None,
        
        # --- åˆ†ç±»å¤´å¯é€‰å‚æ•° ---
        cls_query_num: int = 16,
        cls_dropout: float = 0.0,
        use_cross_attention: bool = True,
        cls_num_heads: int = 4
    ):
        super().__init__()
        # --- æ£€æŸ¥å’Œæ ¼å¼åŒ– nnU-Net å‚æ•° ---
        if isinstance(n_blocks_per_stage, int):
            n_blocks_per_stage = [n_blocks_per_stage] * n_stages
        if isinstance(n_conv_per_stage_decoder, int):
            n_conv_per_stage_decoder = [n_conv_per_stage_decoder] * (n_stages - 1)
        if norm_op_kwargs is None:
            norm_op_kwargs = {'eps': 1e-5, 'affine': True}
        if nonlin_kwargs is None:
            nonlin_kwargs = {'inplace': True}
            
        self.deep_supervision = deep_supervision
        self.num_classes = num_classes # åˆ†å‰²ç±»åˆ«æ•°
        self.n_stages = n_stages

        # ---------------------------------
        # 1. æ„å»ºå…±äº« 3D ResNet ç¼–ç å™¨
        # ---------------------------------
        self.conv_encoder_blocks = nn.ModuleList()
        if stem_channels is None: 
            stem_channels = features_per_stage[0]
        
        self.conv_encoder_blocks.append(
            StackedResidualBlocks(
                n_blocks_per_stage[0], conv_op, input_channels, stem_channels, kernel_sizes[0], strides[0],
                conv_bias, norm_op, norm_op_kwargs, dropout_op, dropout_op_kwargs, 
                nonlin, nonlin_kwargs, block=block
            )
        )
        for s in range(1, n_stages):
            self.conv_encoder_blocks.append(
                StackedResidualBlocks(
                    n_blocks_per_stage[s], conv_op, features_per_stage[s - 1], features_per_stage[s], 
                    kernel_sizes[s], strides[s], conv_bias, norm_op, norm_op_kwargs, 
                    dropout_op, dropout_op_kwargs, nonlin, nonlin_kwargs, block=block
                )
            )

        # ---------------------------------
        # 2. æ„å»ºåˆ†å‰²è§£ç å™¨ (Seg Head)
        # ---------------------------------
        self.transpconvs = nn.ModuleList()
        self.decoder_blocks = nn.ModuleList()
        self.seg_layers = nn.ModuleList()
        transpconv_op = get_matching_convtransp(conv_op)
        
        for s in range(n_stages - 1, 0, -1):
            self.transpconvs.append(
                transpconv_op(
                    features_per_stage[s], features_per_stage[s - 1], strides[s], strides[s],
                    bias=conv_bias
                )
            )
            decoder_input_features = 2 * features_per_stage[s - 1]
            self.decoder_blocks.append(
                StackedConvBlocks(
                    n_conv_per_stage_decoder[s - 1], conv_op, decoder_input_features, features_per_stage[s - 1],
                    kernel_sizes[s - 1], 1, conv_bias, norm_op, norm_op_kwargs,
                    dropout_op, dropout_op_kwargs, nonlin, nonlin_kwargs
                )
            )
            self.seg_layers.append(
                conv_op(features_per_stage[s - 1], num_classes, 1, 1, 0, bias=True)
            )

        # ---------------------------------
        # 3. â˜…â˜…â˜… æ„å»ºæ–°çš„èšåˆåˆ†ç±»å¤´ (Cls Head) â˜…â˜…â˜…
        # ---------------------------------
        
        # ç“¶é¢ˆå±‚çš„ç‰¹å¾æ•°
        bottleneck_features_dim = features_per_stage[-1]
        
        # æ€»ç‰¹å¾ç»´åº¦ = ç¼–ç å™¨æ‰€æœ‰é˜¶æ®µ + ç“¶é¢ˆå±‚ çš„ç‰¹å¾æ€»å’Œ
        total_cls_input_dim = sum(features_per_stage)

        # --- æ–°å¢ï¼šä¸ºåˆ†ç±»ä»»åŠ¡æ·»åŠ ä¸€ä¸ªâ€œé€‚é…å™¨â€ ---
        # è¿™ä¸ªæ¨¡å—å°†ç“¶é¢ˆç‰¹å¾å›¾è½¬æ¢ä¸ºåˆ†ç±»ç‰¹å¾å›¾
        self.cls_adapter = nn.Sequential(
            StackedConvBlocks(
                2, conv_op, bottleneck_features_dim, bottleneck_features_dim,
                kernel_sizes[-1], 1, conv_bias, norm_op, norm_op_kwargs,
                dropout_op, dropout_op_kwargs, nonlin, nonlin_kwargs
            ),
            # ä½ å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œä½¿ç”¨ 1x1x1 å·ç§¯æ¥æ”¹å˜ç»´åº¦ï¼Œ
            # ä½†ä¿æŒç»´åº¦ä¸€è‡´é€šå¸¸æ˜¯å®‰å…¨çš„ã€‚
            conv_op(bottleneck_features_dim, total_cls_input_dim, 1, 1, 0, bias=True)
        )
        

        # ä½ çš„ classification_head å®šä¹‰ä¿æŒä¸å˜
        # (æ— è®ºæ˜¯ä½¿ç”¨ CrossAttention è¿˜æ˜¯ç®€å•çš„ MLP)
        self.classification_head = nn.Sequential(
        nn.Linear(total_cls_input_dim, total_cls_input_dim // 2),
        nn.ReLU(inplace=True),
        nn.Dropout(cls_dropout),
        nn.Linear(total_cls_input_dim // 2, cls_num_classes)
        )

    def forward(self, x: torch.Tensor) -> Tuple[Union[torch.Tensor, List[torch.Tensor]], torch.Tensor]:
        
        skips = []
        cls_feature_list = [] # ç”¨äº PANDA/iAorta ç­–ç•¥çš„ç‰¹å¾åˆ—è¡¨
        
        # ---------------------------------
        # 1. & 2. ç¼–ç å™¨ + ç“¶é¢ˆå±‚
        # ---------------------------------
        # self.conv_encoder_blocks åˆ—è¡¨åŒ…å«äº† stem å’Œæ‰€æœ‰ encoder é˜¶æ®µ
        # (å…± n_stages ä¸ªå—)
        x_enc = x
        
        # è¿­ä»£ n_stages æ¬¡ (ä¾‹å¦‚ 6 æ¬¡)
        # self.n_stages æ˜¯åœ¨ __init__ ä¸­å®šä¹‰çš„
        for i in range(self.n_stages): 
            x_enc = self.conv_encoder_blocks[i](x_enc)
            skips.append(x_enc)
            cls_feature_list.append(x_enc) 
            # cls_feature_list ç°åœ¨åŒ…å«äº† stem, stage 1, ..., stage 5 (ç“¶é¢ˆå±‚)
            # çš„æ‰€æœ‰ (n_stages) ä¸ªç‰¹å¾å›¾
        
        # ç“¶é¢ˆå±‚ç‰¹å¾å·²ç»æ˜¯ skips åˆ—è¡¨çš„æœ€åä¸€é¡¹
        bottleneck_features = skips[-1] 
        # (ä¸éœ€è¦å†å‘ cls_feature_list æ·»åŠ ä»»ä½•ä¸œè¥¿)

        # ---------------------------------
        # 3. åˆ†å‰²è§£ç å™¨ (Seg Decoder)
        # ---------------------------------
        seg_outputs = []
        x_dec = bottleneck_features
        
        # è¿­ä»£ (n_stages - 1) æ¬¡ (ä¾‹å¦‚ 5 æ¬¡)
        for i in range(len(self.decoder_blocks)):
            # ç¼–ç å™¨æœ‰ n_stages ä¸ªè¾“å‡º (skips åˆ—è¡¨)
            # è§£ç å™¨æœ‰ (n_stages - 1) ä¸ªå—
            # ç¬¬ i ä¸ªè§£ç å™¨å— (i=0..n_stages-2)
            # éœ€è¦è¿æ¥ç¬¬ (n_stages - 2 - i) ä¸ª skip
            # è¿™ç­‰äº skips[-(i + 2)]
            skip_connection = skips[-(i + 2)] 
            x_dec = self.transpconvs[i](x_dec)
            x_dec = torch.cat((x_dec, skip_connection), dim=1)
            x_dec = self.decoder_blocks[i](x_dec)
            
            seg_outputs.append(self.seg_layers[i](x_dec))

        # ---------------------------------
        # 4. â˜…â˜…â˜… å®Œæˆåˆ†ç±»å¤´çš„è®¡ç®— â˜…â˜…â˜…
        # ---------------------------------
        # (d) cls_feature_list å·²ç»åŒ…å«äº†æ‰€æœ‰ç¼–ç å™¨/ç“¶é¢ˆå±‚çš„ç‰¹å¾å›¾
        #    å®ƒçš„é•¿åº¦åº”è¯¥ç­‰äº n_stages
        pooled_features = [f.mean(dim=[-1, -2, -3]) for f in cls_feature_list]
        
        # (e) æ‹¼æ¥æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾å‘é‡
        combined_vector = torch.cat(pooled_features, dim=1)

        # æ·»åŠ  cls_adapter è°ƒç”¨ï¼ˆ
        adapted_feature = self.cls_adapter(cls_feature_list[-1])  # <--- ğŸ”¥ ç”¨ç“¶é¢ˆå±‚ç‰¹å¾é€‚é…
        class_input = adapted_feature.mean(dim=[-1, -2, -3])   # shape: [B, 1120]
        # (f) å¾—åˆ°æœ€ç»ˆåˆ†ç±»è¾“å‡º
        #     self.classification_head æ˜¯ä½ åœ¨ __init__ ä¸­å®šä¹‰çš„ MLP
        # class_output = self.classification_head(combined_vector)
        class_output = self.classification_head(class_input)

        # ---------------------------------
        # 5. æ ¼å¼åŒ–åˆ†å‰²è¾“å‡º (ä¿æŒä¸å˜)
        # ---------------------------------
        if self.deep_supervision:
            seg_output_return = seg_outputs[::-1] # åè½¬åˆ—è¡¨ä»¥åŒ¹é… nnU-Net æ·±åº¦ç›‘ç£
        else:
            seg_output_return = seg_outputs[-1] 
            
        return seg_output_return, class_output

if __name__ == '__main__':
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # --- 1. å®šä¹‰ nnU-Net v2 çš„æ ‡å‡† "3d_fullres" ResNet é…ç½® ---
    
    # ä»»åŠ¡1ï¼šåˆ†å‰² (å‡è®¾14ä¸ªç±»åˆ«ï¼ŒåŒ…æ‹¬èƒŒæ™¯)
    seg_classes = 14 
    
    # ä»»åŠ¡2ï¼šåˆ†ç±» (å‡è®¾3ä¸ªç±»åˆ«)
    cls_classes = 3 
    
    # nnU-Net v2 å…¸å‹é…ç½®
    unet_config = {
        "input_channels": 1,
        "n_stages": 6,
        "features_per_stage": (32, 64, 128, 256, 320, 320),
        "conv_op": nn.Conv3d,
        "kernel_sizes": ((3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3), (3, 3, 3)),
        "strides": ((1, 1, 1), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2), (2, 2, 2)),
        "n_blocks_per_stage": (2, 2, 2, 2, 2, 2),
        "num_classes": seg_classes, # ä¼ å…¥åˆ†å‰²ç±»åˆ«æ•°
        "n_conv_per_stage_decoder": (2, 2, 2, 2, 2),
        "conv_bias": True,
        "norm_op": nn.InstanceNorm3d,
        "norm_op_kwargs": {"eps": 1e-05, "affine": True},
        "dropout_op": None,
        "dropout_op_kwargs": None,
        "nonlin": nn.LeakyReLU,
        "nonlin_kwargs": {"inplace": True},
        "deep_supervision": True,
        "block": BasicBlockD, # è¿™ä½¿å…¶æˆä¸º ResNet ç¼–ç å™¨
    }

    # --- 2. å®ä¾‹åŒ–æˆ‘ä»¬çš„å¤šä»»åŠ¡æ¨¡å‹ ---
    # *******************************************************
    # *** å”¯ä¸€çš„æ›´æ”¹ï¼šä½¿ç”¨æˆ‘ä»¬æ–°çš„ã€ä» nn.Module ç»§æ‰¿çš„ç±» ***
    # *******************************************************
    model = ResNet_MTL_nnUNet(
        **unet_config, # ä¼ å…¥æ‰€æœ‰ nnU-Net æ ‡å‡†å‚æ•°
        
        # ä¼ å…¥åˆ†ç±»å¤´çš„ç‰¹å®šå‚æ•°
        cls_num_classes=cls_classes,
        cls_query_num=16,          # ä½¿ç”¨ 16 ä¸ª query "æ¢é’ˆ"
        cls_dropout=0.1,
        use_cross_attention=True
    ).to(device)

    # --- 3. æµ‹è¯•å‰å‘ä¼ æ’­ ---
    
    dummy_input = torch.randn(2, 1, 128, 128, 128).to(device) 

    with torch.no_grad():
        seg_output, class_output = model(dummy_input)

    print(f"Input shape: {dummy_input.shape}")
    print("-" * 30)
    print(f"Classification output shape: {class_output.shape}")
    print("-" * 30)
    
    print("Segmentation outputs (Deep Supervision):")
    if isinstance(seg_output, list):
        for i, out in enumerate(seg_output):
            print(f"  Level {i} (High-res to Low-res): {out.shape}")
    else:
        print(f"  Output shape: {seg_output.shape}")